{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a25b54e",
   "metadata": {
    "papermill": {
     "duration": 0.010105,
     "end_time": "2023-06-11T11:58:06.668210",
     "exception": false,
     "start_time": "2023-06-11T11:58:06.658105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Baseline submission\n",
    "\n",
    "A notebook to generate a valid submission. Implements three local feature/matcher methods: LoFTR, DISK, and KeyNetAffNetHardNet.\n",
    "\n",
    "Remember to enable a GPU accelerator and disable internet access, then press \"submit\" on the right pane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c2e886",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:06.689227Z",
     "iopub.status.busy": "2023-06-11T11:58:06.688228Z",
     "iopub.status.idle": "2023-06-11T11:58:12.705668Z",
     "shell.execute_reply": "2023-06-11T11:58:12.704259Z"
    },
    "papermill": {
     "duration": 6.031784,
     "end_time": "2023-06-11T11:58:12.709097",
     "exception": false,
     "start_time": "2023-06-11T11:58:06.677313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "# 3D reconstruction\n",
    "# import pvsac\n",
    "import pycolmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b48060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.728851Z",
     "iopub.status.busy": "2023-06-11T11:58:12.728478Z",
     "iopub.status.idle": "2023-06-11T11:58:12.733467Z",
     "shell.execute_reply": "2023-06-11T11:58:12.732389Z"
    },
    "papermill": {
     "duration": 0.017387,
     "end_time": "2023-06-11T11:58:12.735911",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.718524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ea5aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.755767Z",
     "iopub.status.busy": "2023-06-11T11:58:12.754821Z",
     "iopub.status.idle": "2023-06-11T11:58:12.762122Z",
     "shell.execute_reply": "2023-06-11T11:58:12.760963Z"
    },
    "papermill": {
     "duration": 0.020259,
     "end_time": "2023-06-11T11:58:12.764936",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.744677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.8.1\n",
      "Pycolmap version 3.11.1\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)\n",
    "\n",
    "LOCAL_FEATURE = 'LoFTR'\n",
    "device=torch.device('cuda')\n",
    "# Can be LoFTR, KeyNetAffNetHardNet, or DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dda0a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.785098Z",
     "iopub.status.busy": "2023-06-11T11:58:12.784066Z",
     "iopub.status.idle": "2023-06-11T11:58:12.791981Z",
     "shell.execute_reply": "2023-06-11T11:58:12.790821Z"
    },
    "papermill": {
     "duration": 0.020678,
     "end_time": "2023-06-11T11:58:12.794573",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.773895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.image_to_tensor(cv2.imread(fname), False).float() / 255.\n",
    "    img = K.color.bgr_to_rgb(img.to(device))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa7b9c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.815133Z",
     "iopub.status.busy": "2023-06-11T11:58:12.814437Z",
     "iopub.status.idle": "2023-06-11T11:58:12.839333Z",
     "shell.execute_reply": "2023-06-11T11:58:12.838276Z"
    },
    "papermill": {
     "duration": 0.038057,
     "end_time": "2023-06-11T11:58:12.842042",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.803985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "\n",
    "def get_global_desc(fnames, model,\n",
    "                    device =  torch.device('cpu')):\n",
    "    model = model.eval()\n",
    "    model= model.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "    global_descs_convnext=[]\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "#         img = Image.open(img_fname_full).convert('RGB')\n",
    "        img = cv2.imread(img_fname_full)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (512, 512))\n",
    "        img = Image.fromarray(img)\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #print (desc.shape)\n",
    "            desc = desc.view(1, -1)\n",
    "            desc_lr = model.forward_features(timg.flip(-1).to(device)).mean(dim=(-1,2))#\n",
    "            #print (desc.shape)\n",
    "            desc_lr = desc_lr.view(1, -1)\n",
    "            desc_norm = F.normalize((desc+desc_lr)/2, dim=1, p=2)\n",
    "        #print (desc_norm)\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    \n",
    "    model_path = ['/kaggle/input/tf-efficientnet-b7/tf_efficientnet_b7_ra-6c08e654.pth']\n",
    "    model_name = ['tf_efficientnet_b7']\n",
    "    descs_list = []\n",
    "    for i in range(len(model_name)): \n",
    "        model = timm.create_model(model_name[i], \n",
    "                                  checkpoint_path=model_path[i])\n",
    "        model.eval()\n",
    "        descs = get_global_desc(fnames, model, device=device)\n",
    "        descs_list.append(descs)\n",
    "        \n",
    "    descs = torch.cat(descs_list, dim=-1)\n",
    "    print(descs.shape)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb037b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.861764Z",
     "iopub.status.busy": "2023-06-11T11:58:12.861383Z",
     "iopub.status.idle": "2023-06-11T11:58:12.899683Z",
     "shell.execute_reply": "2023-06-11T11:58:12.898495Z"
    },
    "papermill": {
     "duration": 0.051279,
     "end_time": "2023-06-11T11:58:12.902280",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.851001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to manipulate a colmap database.\n",
    "# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
    "\n",
    "# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "#     * Redistributions of source code must retain the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer.\n",
    "#\n",
    "#     * Redistributions in binary form must reproduce the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer in the\n",
    "#       documentation and/or other materials provided with the distribution.\n",
    "#\n",
    "#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n",
    "#       its contributors may be used to endorse or promote products derived\n",
    "#       from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n",
    "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "# POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n",
    "\n",
    "# This script is based on an original implementation by True Price.\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed1b55f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.923613Z",
     "iopub.status.busy": "2023-06-11T11:58:12.922518Z",
     "iopub.status.idle": "2023-06-11T11:58:12.951396Z",
     "shell.execute_reply": "2023-06-11T11:58:12.950264Z"
    },
    "papermill": {
     "duration": 0.043063,
     "end_time": "2023-06-11T11:58:12.954342",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.911279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to interface DISK with Colmap.\n",
    "# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n",
    "\n",
    "#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6ba4af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:12.976112Z",
     "iopub.status.busy": "2023-06-11T11:58:12.974302Z",
     "iopub.status.idle": "2023-06-11T11:58:12.987495Z",
     "shell.execute_reply": "2023-06-11T11:58:12.986449Z"
    },
    "papermill": {
     "duration": 0.026238,
     "end_time": "2023-06-11T11:58:12.989998",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.963760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making kornia local features loading w/o internet\n",
    "class KeyNetAffNetHardNet(KF.LocalFeature):\n",
    "    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n",
    "\n",
    "    .. image:: _static/img/keynet_affnet.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 5000,\n",
    "        upright: bool = False,\n",
    "        device = torch.device('cpu'),\n",
    "        scale_laf: float = 1.0,\n",
    "    ):\n",
    "        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n",
    "        if not upright:\n",
    "            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n",
    "            ori_module.angle_detector.load_state_dict(weights)\n",
    "        detector = KF.KeyNetDetector(\n",
    "            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n",
    "        ).to(device)\n",
    "        kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n",
    "        detector.model.load_state_dict(kn_weights)\n",
    "        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n",
    "        detector.aff.load_state_dict(affnet_weights)\n",
    "        \n",
    "        hardnet = KF.HardNet(False).eval()\n",
    "        hn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n",
    "        hardnet.load_state_dict(hn_weights)\n",
    "        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n",
    "        super().__init__(detector, descriptor, scale_laf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d77b397f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:13.010430Z",
     "iopub.status.busy": "2023-06-11T11:58:13.010048Z",
     "iopub.status.idle": "2023-06-11T11:58:13.083671Z",
     "shell.execute_reply": "2023-06-11T11:58:13.082508Z"
    },
    "papermill": {
     "duration": 0.087898,
     "end_time": "2023-06-11T11:58:13.086590",
     "exception": false,
     "start_time": "2023-06-11T11:58:12.998692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You may want to enable the GPU switch?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "from glob import glob\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    print('You may want to enable the GPU switch?')\n",
    "\n",
    "INSTALLED_LOG = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37e645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:13.107337Z",
     "iopub.status.busy": "2023-06-11T11:58:13.106946Z",
     "iopub.status.idle": "2023-06-11T11:58:19.456711Z",
     "shell.execute_reply": "2023-06-11T11:58:19.455212Z"
    },
    "papermill": {
     "duration": 6.36376,
     "end_time": "2023-06-11T11:58:19.459695",
     "exception": false,
     "start_time": "2023-06-11T11:58:13.095935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "302.96s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /tmp/superpoint: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "308.41s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: ../input/super-glue-pretrained-network/models: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313.82s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: /tmp/superpoint/superpoint: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319.22s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "touch: /tmp/superpoint/superpoint/__init__.py: No such file or directory\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'superpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/superpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msuperpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msuperpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SuperPoint\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msuperpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msuperglue\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SuperGlue\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSuperGlueCustomMatchingV2\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'superpoint'"
     ]
    }
   ],
   "source": [
    "# Install superglue\n",
    "force_superglue_reinstall = False\n",
    "\n",
    "if 'superglue' not in INSTALLED_LOG or force_superglue_reinstall:\n",
    "    !mkdir /tmp/superpoint\n",
    "    !cp -r ../input/super-glue-pretrained-network/models /tmp/superpoint/superpoint\n",
    "    !ls /tmp/superpoint/superpoint\n",
    "    !touch /tmp/superpoint/superpoint/__init__.py\n",
    "    INSTALLED_LOG['superglue'] = True\n",
    "else:\n",
    "    print('Already installed SuperGlue. Set \"force_superglue_reinstall=True\" to override this behavior.')\n",
    "\n",
    "    # https://www.kaggle.com/datasets/losveria/super-glue-pretrained-network !!! SOURCE OF SUPERPOINT AND SUPERGLUE\n",
    "\n",
    "# Import superglue\n",
    "import sys\n",
    "sys.path.append(\"/tmp/superpoint\")\n",
    "from superpoint.superpoint import SuperPoint\n",
    "from superpoint.superglue import SuperGlue\n",
    "\n",
    "\n",
    "class SuperGlueCustomMatchingV2(torch.nn.Module):\n",
    "    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n",
    "    def __init__(self, config={}, device=None):\n",
    "        super().__init__()\n",
    "        self.superpoint = SuperPoint(config.get('superpoint', {}))\n",
    "        self.superglue = SuperGlue(config.get('superglue', {}))\n",
    "\n",
    "        self.tta_map = {\n",
    "            'orig': self.untta_none,\n",
    "            'eqhist': self.untta_none,\n",
    "            'clahe': self.untta_none,\n",
    "            'flip_lr': self.untta_fliplr,\n",
    "            'flip_ud': self.untta_flipud,\n",
    "            'rot_r10': self.untta_rotr10,\n",
    "            'rot_l10': self.untta_rotl10,\n",
    "            'fliplr_rotr10': self.untta_fliplr_rotr10,\n",
    "            'fliplr_rotl10': self.untta_fliplr_rotl10\n",
    "        }\n",
    "        self.device = device\n",
    "\n",
    "    def forward_flat(self, data, ttas=['orig', ], tta_groups=[['orig']]):\n",
    "        \"\"\" Run SuperPoint (optionally) and SuperGlue\n",
    "        SuperPoint is skipped if ['keypoints0', 'keypoints1'] exist in input\n",
    "        Args:\n",
    "          data: dictionary with minimal keys: ['image0', 'image1']\n",
    "        \"\"\"\n",
    "        pred = {}\n",
    "\n",
    "        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n",
    "        # sp_st = time.time()\n",
    "        if 'keypoints0' not in data:\n",
    "            pred0 = self.superpoint({'image': data['image0']})\n",
    "            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n",
    "        if 'keypoints1' not in data:\n",
    "            pred1 = self.superpoint({'image': data['image1']})\n",
    "            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n",
    "        # sp_nd = time.time()\n",
    "        # print('SP:', sp_nd - sp_st, 's')\n",
    "\n",
    "        # Reverse-tta before inference\n",
    "        pred['scores0'] = list(pred['scores0'])\n",
    "        pred['scores1'] = list(pred['scores1'])\n",
    "        for i in range(len(pred['keypoints0'])):\n",
    "            pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i] = self.tta_map[ttas[i]](\n",
    "                pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i],\n",
    "                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=True)\n",
    "\n",
    "            pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i] = self.tta_map[ttas[i]](\n",
    "                pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i],\n",
    "                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=True)\n",
    "\n",
    "        # Batch all features\n",
    "        # We should either have i) one image per batch, or\n",
    "        # ii) the same number of local features for all images in the batch.\n",
    "        data = {**data, **pred}\n",
    "\n",
    "        group_preds = []\n",
    "        for tta_group in tta_groups:\n",
    "            group_mask = torch.from_numpy(np.array([x in tta_group for x in ttas], dtype=np.bool))\n",
    "            group_data = {\n",
    "                **{f'keypoints{k}': [data[f'keypoints{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n",
    "                **{f'descriptors{k}': [data[f'descriptors{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n",
    "                **{f'scores{k}': [data[f'scores{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n",
    "                **{f'image{k}': data[f'image{k}'][group_mask, ...] for k in [0, 1]},\n",
    "            }\n",
    "            for k, v in group_data.items():\n",
    "                if isinstance(group_data[k], (list, tuple)):\n",
    "                    if k.startswith('descriptor'):\n",
    "                        group_data[k] = torch.cat(group_data[k], 1)[None, ...]\n",
    "                    else:\n",
    "                        group_data[k] = torch.cat(group_data[k])[None, ...]\n",
    "                else:\n",
    "                    group_data[k] = torch.flatten(group_data[k], 0, 1)[None, ...]\n",
    "            # sg_st = time.time()\n",
    "            group_pred = {\n",
    "                # **{k: group_data[k] for k in group_data},\n",
    "                **group_data,\n",
    "                **self.superglue(group_data)\n",
    "            }\n",
    "            # sg_nd = time.time()\n",
    "            # print('SG:', sg_nd - sg_st, 's')\n",
    "            group_preds.append(group_pred)\n",
    "        return group_preds\n",
    "\n",
    "    def forward_cross(self, data, ttas=['orig', ], tta_groups=[('orig', 'orig')]):\n",
    "        pred = {}\n",
    "\n",
    "        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n",
    "        sp_st = time()\n",
    "        if 'keypoints0' not in data:\n",
    "            pred0 = self.superpoint({'image': data['image0']})\n",
    "            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n",
    "        if 'keypoints1' not in data:\n",
    "            pred1 = self.superpoint({'image': data['image1']})\n",
    "            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n",
    "        sp_nd = time()\n",
    "\n",
    "        # Batch all features\n",
    "        # We should either have i) one image per batch, or\n",
    "        # ii) the same number of local features for all images in the batch.\n",
    "        data = {**data, **pred}\n",
    "\n",
    "        # Group predictions (list, with elements with matches{0,1}, matching_scores{0,1} keys)\n",
    "        group_pred_list = []\n",
    "        tta2id = {k: i for i, k in enumerate(ttas)}\n",
    "        for tta_group in tta_groups:\n",
    "            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n",
    "            group_data = {\n",
    "                **{f'image{i}': data[f'image{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'descriptors{i}': data[f'descriptors{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "            }\n",
    "\n",
    "            for k in group_data:\n",
    "                if isinstance(group_data[k], (list, tuple)):\n",
    "                    group_data[k] = torch.stack(group_data[k])\n",
    "\n",
    "            group_sg_pred = self.superglue(group_data)\n",
    "            group_pred_list.append(group_sg_pred)\n",
    "\n",
    "        # UnTTA\n",
    "        data['scores0'] = list(data['scores0'])\n",
    "        data['scores1'] = list(data['scores1'])\n",
    "        for i in range(len(data['keypoints0'])):\n",
    "            data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i] = self.tta_map[ttas[i]](\n",
    "                data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i],\n",
    "                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=False)\n",
    "\n",
    "            data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i] = self.tta_map[ttas[i]](\n",
    "                data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i],\n",
    "                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=False)\n",
    "\n",
    "        # Sooo... groups?\n",
    "        for group_pred, tta_group in zip(group_pred_list, tta_groups):\n",
    "            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n",
    "            group_pred.update({\n",
    "                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "            })\n",
    "        return group_pred_list\n",
    "\n",
    "\n",
    "    def untta_none(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        if not inplace:\n",
    "            keypoints = keypoints.clone()\n",
    "        return keypoints, descriptors, scores\n",
    "    \n",
    "    def untta_fliplr(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        if not inplace:\n",
    "            keypoints = keypoints.clone()\n",
    "        keypoints[:, 0] = w - keypoints[:, 0] - 1.\n",
    "        return keypoints, descriptors, scores\n",
    "\n",
    "    def untta_flipud(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        if not inplace:\n",
    "            keypoints = keypoints.clone()\n",
    "        keypoints[:, 1] = h - keypoints[:, 1] - 1.\n",
    "        return keypoints, descriptors, scores\n",
    "\n",
    "    def untta_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is +10, inverse is -10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "\n",
    "    def untta_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is -10, inverse is +10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "        \n",
    "    def untta_fliplr_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is +10, inverse is -10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "\n",
    "    def untta_fliplr_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is -10, inverse is +10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "\n",
    "\n",
    "class SuperGlueMatcherV2:\n",
    "    def __init__(self, config, device=None, conf_th=None):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self._superglue_matcher = SuperGlueCustomMatchingV2(\n",
    "            config=config, device=self.device,\n",
    "            ).eval().to(device)\n",
    "        self.conf_thresh = conf_th\n",
    "    \n",
    "    def prep_np_img(self, img, long_side=None):\n",
    "        if long_side is not None:\n",
    "            scale = long_side / max(img.shape[0], img.shape[1])\n",
    "            w = int(img.shape[1] * scale)\n",
    "            h = int(img.shape[0] * scale)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), scale\n",
    "    \n",
    "    def frame2tensor(self, frame):\n",
    "        return (torch.from_numpy(frame).float()/255.)[None, None].to(self.device)\n",
    "            \n",
    "    def tta_rotation_preprocess(self, img_np, angle):\n",
    "        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n",
    "        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n",
    "        rot_img = self.frame2tensor(cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0])))\n",
    "        return rot_M, rot_img, rot_M_inv\n",
    "\n",
    "    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n",
    "        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n",
    "        hom = np.concatenate([kpts, ones], 1)\n",
    "        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n",
    "        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n",
    "        return rot_kpts, mask\n",
    "# \n",
    "    def __call__(self, img_np0, img_np1, input_longside, tta_groups=[('orig', 'orig')], forward_type='cross'):\n",
    "        with torch.no_grad():\n",
    "            img_np0, scale0 = self.prep_np_img(img_np0, input_longside)\n",
    "            img_np1, scale1 = self.prep_np_img(img_np1, input_longside)\n",
    "\n",
    "            img_ts0 = self.frame2tensor(img_np0)\n",
    "            img_ts1 = self.frame2tensor(img_np1)\n",
    "            images0, images1 = [], []\n",
    "\n",
    "            tta = []\n",
    "            for tta_g in tta_groups:\n",
    "                tta += tta_g\n",
    "            tta = list(set(tta))\n",
    "\n",
    "            # TTA\n",
    "            for tta_elem in tta:\n",
    "                if tta_elem == 'orig':\n",
    "                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n",
    "                elif tta_elem == 'flip_lr':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n",
    "                elif tta_elem == 'flip_ud':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n",
    "                elif tta_elem == 'rot_r10':\n",
    "                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0, 15)\n",
    "                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1, 15)\n",
    "                elif tta_elem == 'rot_l10':\n",
    "                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0, -15)\n",
    "                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1, -15)\n",
    "                elif tta_elem == 'fliplr_rotr10':\n",
    "                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], 15)\n",
    "                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], 15)\n",
    "                elif tta_elem == 'fliplr_rotl10':\n",
    "                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], -15)\n",
    "                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], -15)\n",
    "                elif tta_elem == 'eqhist':\n",
    "                    img_ts0_aug = self.frame2tensor(cv2.equalizeHist(img_np0))\n",
    "                    img_ts1_aug = self.frame2tensor(cv2.equalizeHist(img_np1))\n",
    "                elif tta_elem == 'clahe':\n",
    "                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "                    img_ts0_aug = self.frame2tensor(clahe.apply(img_np0))\n",
    "                    img_ts1_aug = self.frame2tensor(clahe.apply(img_np1))\n",
    "                else:\n",
    "                    raise ValueError('Unknown TTA method.')\n",
    "\n",
    "                images0.append(img_ts0_aug)\n",
    "                images1.append(img_ts1_aug)\n",
    "\n",
    "            # Inference\n",
    "            if forward_type == 'cross':\n",
    "                pred = self._superglue_matcher.forward_cross(\n",
    "                    data={\n",
    "                        \"image0\": torch.cat(images0),\n",
    "                        \"image1\": torch.cat(images1)\n",
    "                    },\n",
    "                    ttas=tta, tta_groups=tta_groups)\n",
    "            elif forward_type == 'flat':\n",
    "                pred = self._superglue_matcher.forward_flat(\n",
    "                data={\n",
    "                    \"image0\": torch.cat(images0),\n",
    "                    \"image1\": torch.cat(images1)\n",
    "                },\n",
    "                ttas=tta, tta_groups=tta_groups)\n",
    "            else:\n",
    "                raise RuntimeError(f'Unknown forward_type {forward_type}')\n",
    "\n",
    "            mkpts0, mkpts1, mconf = [], [], []\n",
    "            for group_pred in pred:\n",
    "                pred_aug = {k: v[0].detach().cpu().numpy().squeeze() for k, v in group_pred.items()}\n",
    "                kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n",
    "                matches, conf = pred_aug[\"matches0\"], pred_aug[\"matching_scores0\"]\n",
    "\n",
    "                if self.conf_thresh is None:\n",
    "                    valid = matches > -1\n",
    "                else:\n",
    "                    valid = (matches > -1) & (conf >= self.conf_thresh)\n",
    "                mkpts0.append(kpts0[valid])\n",
    "                mkpts1.append(kpts1[matches[valid]])\n",
    "                mconf.append(conf[valid])\n",
    "\n",
    "            cat_mkpts0 = np.concatenate(mkpts0)\n",
    "            cat_mkpts1 = np.concatenate(mkpts1)\n",
    "            mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < img_np0.shape[1]) & (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < img_np0.shape[0])\n",
    "            mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < img_np1.shape[1]) & (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < img_np1.shape[0])\n",
    "            return cat_mkpts0[mask0 & mask1] / scale0, cat_mkpts1[mask0 & mask1] / scale1\n",
    "\n",
    "\n",
    "# 1600 is the validation size in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d9b76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:19.480317Z",
     "iopub.status.busy": "2023-06-11T11:58:19.479941Z",
     "iopub.status.idle": "2023-06-11T11:58:19.518712Z",
     "shell.execute_reply": "2023-06-11T11:58:19.517510Z"
    },
    "papermill": {
     "duration": 0.052342,
     "end_time": "2023-06-11T11:58:19.521493",
     "exception": false,
     "start_time": "2023-06-11T11:58:19.469151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoFTRMatcher:\n",
    "    def __init__(self, device=None, input_longside=1200, conf_th=None):\n",
    "        self._loftr_matcher = KF.LoFTR(pretrained=None)\n",
    "        self._loftr_matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n",
    "        self._loftr_matcher = self._loftr_matcher.to(device).eval()\n",
    "        self.device = device\n",
    "        self.conf_thresh = conf_th\n",
    "        \n",
    "    def prep_img(self, img, long_side=1200):\n",
    "        if long_side is not None:\n",
    "            scale = long_side / max(img.shape[0], img.shape[1]) \n",
    "            w = int(img.shape[1] * scale)\n",
    "            h = int(img.shape[0] * scale)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        img_ts = K.image_to_tensor(img, False).float() / 255.\n",
    "        img_ts = K.color.bgr_to_rgb(img_ts)\n",
    "        img_ts = K.color.rgb_to_grayscale(img_ts)\n",
    "        return img, img_ts.to(self.device), scale\n",
    "    \n",
    "    def tta_rotation_preprocess(self, img_np, angle):\n",
    "        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n",
    "        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n",
    "        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n",
    "\n",
    "        rot_img_ts = K.image_to_tensor(rot_img, False).float() / 255.\n",
    "        rot_img_ts = K.color.bgr_to_rgb(rot_img_ts)\n",
    "        rot_img_ts = K.color.rgb_to_grayscale(rot_img_ts)\n",
    "        return rot_M, rot_img_ts.to(self.device), rot_M_inv\n",
    "\n",
    "    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n",
    "        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n",
    "        hom = np.concatenate([kpts, ones], 1)\n",
    "        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n",
    "        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n",
    "        return rot_kpts, mask\n",
    "# \n",
    "    def __call__(self, img_np1, img_np2, input_longside, tta=['orig', 'flip_lr']):\n",
    "        with torch.no_grad():\n",
    "            img_np1, img_ts0, scale0 = self.prep_img(img_np1, input_longside)\n",
    "            img_np2, img_ts1, scale1 = self.prep_img(img_np2, input_longside)\n",
    "            images0, images1 = [], []\n",
    "\n",
    "            # TTA\n",
    "            for tta_elem in tta:\n",
    "                if tta_elem == 'orig':\n",
    "                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n",
    "                elif tta_elem == 'flip_lr':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n",
    "                elif tta_elem == 'flip_ud':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n",
    "                elif tta_elem == 'rot_r10':\n",
    "                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np1, 10)\n",
    "                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np2, 10)\n",
    "                elif tta_elem == 'rot_l10':\n",
    "                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np1, -10)\n",
    "                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np2, -10)\n",
    "                else:\n",
    "                    raise ValueError('Unknown TTA method.')\n",
    "                images0.append(img_ts0_aug)\n",
    "                images1.append(img_ts1_aug)\n",
    "\n",
    "            # Inference\n",
    "            input_dict = {\"image0\": torch.cat(images0), \"image1\": torch.cat(images1)}\n",
    "            correspondences = self._loftr_matcher(input_dict)\n",
    "            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "            batch_id = correspondences['batch_indexes'].cpu().numpy()\n",
    "            confidence = correspondences['confidence'].cpu().numpy()\n",
    "\n",
    "            # Reverse TTA\n",
    "            for idx, tta_elem in enumerate(tta):\n",
    "                batch_mask = batch_id == idx\n",
    "\n",
    "                if tta_elem == 'orig':\n",
    "                    pass\n",
    "                elif tta_elem == 'flip_lr':\n",
    "                    mkpts0[batch_mask, 0] = img_np1.shape[1] - mkpts0[batch_mask, 0]\n",
    "                    mkpts1[batch_mask, 0] = img_np2.shape[1] - mkpts1[batch_mask, 0]\n",
    "                elif tta_elem == 'flip_ud':\n",
    "                    mkpts0[batch_mask, 1] = img_np1.shape[0] - mkpts0[batch_mask, 1]\n",
    "                    mkpts1[batch_mask, 1] = img_np2.shape[0] - mkpts1[batch_mask, 1]\n",
    "                elif tta_elem == 'rot_r10':\n",
    "                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_r10_M0_inv)\n",
    "                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_r10_M1_inv)\n",
    "                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n",
    "                elif tta_elem == 'rot_l10':\n",
    "                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_l10_M0_inv)\n",
    "                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_l10_M1_inv)\n",
    "                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n",
    "                else:\n",
    "                    raise ValueError('Unknown TTA method.')\n",
    "                    \n",
    "            if self.conf_thresh is not None:\n",
    "                th_mask = confidence >= self.conf_thresh\n",
    "            else:\n",
    "                th_mask = confidence >= 0.\n",
    "            mkpts0, mkpts1 = mkpts0[th_mask, :], mkpts1[th_mask, :]\n",
    "\n",
    "            # Matching points\n",
    "            return mkpts0 / scale0, mkpts1 / scale1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09181c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:19.541972Z",
     "iopub.status.busy": "2023-06-11T11:58:19.541594Z",
     "iopub.status.idle": "2023-06-11T11:58:24.161431Z",
     "shell.execute_reply": "2023-06-11T11:58:24.160132Z"
    },
    "papermill": {
     "duration": 4.633455,
     "end_time": "2023-06-11T11:58:24.164614",
     "exception": false,
     "start_time": "2023-06-11T11:58:19.531159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "base_config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 3,\n",
    "        \"keypoint_threshold\": 0.005,\n",
    "        \"max_keypoints\": 2048,\n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\",\n",
    "        \"sinkhorn_iterations\": 100,\n",
    "        \"match_threshold\": 0.2,\n",
    "    }\n",
    "}\n",
    "f8000_config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 3,\n",
    "        \"keypoint_threshold\": 0.005,\n",
    "        \"max_keypoints\": 2048*4,\n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\",\n",
    "        \"sinkhorn_iterations\": 100,\n",
    "        \"match_threshold\": 0.2,\n",
    "    }\n",
    "}\n",
    "superglue_matcher = SuperGlueMatcherV2(base_config, device=device, conf_th=0.2)     \n",
    "superglue_matcher_8096 = SuperGlueMatcherV2(f8000_config, device=device, conf_th=0.2)     \n",
    "loftr_matcher = LoFTRMatcher(device=device, conf_th=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be45df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.186150Z",
     "iopub.status.busy": "2023-06-11T11:58:24.185610Z",
     "iopub.status.idle": "2023-06-11T11:58:24.274558Z",
     "shell.execute_reply": "2023-06-11T11:58:24.273408Z"
    },
    "papermill": {
     "duration": 0.103108,
     "end_time": "2023-06-11T11:58:24.277529",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.174421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect_features(img_fnames,\n",
    "                    num_feats = 2048,\n",
    "                    upright = False,\n",
    "                    device=torch.device('cpu'),\n",
    "                    feature_dir = '.featureout',\n",
    "                    resize_small_edge_to = 600):\n",
    "\n",
    "    feature = KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n",
    "         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in progress_bar(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                timg = load_torch_image(img_path, device=device)\n",
    "                H, W = timg.shape[2:]\n",
    "                if resize_small_edge_to is None:\n",
    "                    timg_resized = timg\n",
    "                else:\n",
    "                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n",
    "#                     print(f'Resized {timg.shape} to {timg_resized.shape} (resize_small_edge_to={resize_small_edge_to})')\n",
    "                h, w = timg_resized.shape[2:]\n",
    "\n",
    "                lafs, resps, descs = feature(K.color.rgb_to_grayscale(timg_resized))\n",
    "                lafs[:,:,0,:] *= float(W) / float(w)\n",
    "                lafs[:,:,1,:] *= float(H) / float(h)\n",
    "                desc_dim = descs.shape[-1]\n",
    "                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n",
    "                f_laf[key] = lafs.detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return\n",
    "\n",
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def match_features(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=15, \n",
    "                   force_mutual = True,\n",
    "                   matching_alg='smnn'\n",
    "                  ):\n",
    "    assert matching_alg in ['smnn', 'adalam']\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "                    idx1, idx2 = pair_idx\n",
    "                    fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "                    key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "                    lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n",
    "                    lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n",
    "                    desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "                    desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "                    if matching_alg == 'adalam':\n",
    "                        img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n",
    "                        hw1, hw2 = img1.shape[:2], img2.shape[:2]\n",
    "                        adalam_config = KF.adalam.get_adalam_default_config()\n",
    "                        #adalam_config['orientation_difference_threshold'] = None\n",
    "                        #adalam_config['scale_rate_threshold'] = None\n",
    "                        adalam_config['force_seed_mnn']= False\n",
    "                        adalam_config['search_expansion'] = 16\n",
    "                        adalam_config['ransac_iters'] = 128\n",
    "                        adalam_config['device'] = device\n",
    "                        dists, idxs = KF.match_adalam(desc1, desc2,\n",
    "                                                      lafs1, lafs2, # Adalam takes into account also geometric information\n",
    "                                                      hw1=hw1, hw2=hw2,\n",
    "                                                      config=adalam_config) # Adalam also benefits from knowing image size\n",
    "                    else:\n",
    "                        dists, idxs = KF.match_smnn(desc1, desc2, 0.9)\n",
    "                    if len(idxs)  == 0:\n",
    "                        continue\n",
    "                    # Force mutual nearest neighbors\n",
    "                    if force_mutual:\n",
    "                        first_indices = get_unique_idxs(idxs[:,1])\n",
    "                        idxs = idxs[first_indices]\n",
    "                        dists = dists[first_indices]\n",
    "                    n_matches = len(idxs)\n",
    "                    if False:\n",
    "                        print (f'{key1}-{key2}: {n_matches} matches')\n",
    "                    group  = f_match.require_group(key1)\n",
    "                    if n_matches >= min_matches:\n",
    "                         group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "    return\n",
    "\n",
    "def calc_roi_coords(w, h, mkpts, crop_roi_min_side=100, margin=50):\n",
    "    \"\"\"\n",
    "    mkpts is a list [(x1, y1), (x2, y2), ...],\n",
    "    where x is w axis and y is h axis.\n",
    "    \"\"\"\n",
    "    def wiggle(a, b, minl, bound):\n",
    "        \"\"\"evenly pad a and b so that [a, b) have length minl\"\"\"\n",
    "        if b < a:\n",
    "            a, b = b, a\n",
    "        if minl >= bound:\n",
    "            return 0, bound\n",
    "        if (b - a) >= minl:\n",
    "            return a, b\n",
    "        d = (minl - (b - a))\n",
    "        pad_l, pad_r = d // 2, d - d // 2\n",
    "        na, nb = a - pad_l, b + pad_r\n",
    "        if a < 0:\n",
    "            a, b = 0, minl\n",
    "        if b >= bound:\n",
    "            a, b = bound - minl, bound\n",
    "        return a, b\n",
    "\n",
    "    \n",
    "    left, right = int(np.floor(mkpts[:, 0].min())), int(np.ceil(mkpts[:, 0].max()))\n",
    "    top, bottom = int(np.floor(mkpts[:, 1].min())), int(np.ceil(mkpts[:, 1].max()))\n",
    "    left, right = wiggle(left, right, crop_roi_min_side, w)\n",
    "    top, bottom = wiggle(top, bottom, crop_roi_min_side, h)\n",
    "    left, right = max(0, left - margin), min(right + margin, w)\n",
    "    top, bottom = max(0, top - margin), min(bottom + margin, h)\n",
    "    return left, right, top, bottom\n",
    "\n",
    "def match_loftr_superglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout_loftr',\n",
    "                   device=torch.device('cuda'),\n",
    "                   min_matches=15, resize_to_ = (640, 480)):\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            img1 = cv2.imread(fname1)\n",
    "            img2 = cv2.imread(fname2)\n",
    "            \n",
    "            mkpts0_loftr, mkpts1_loftr = loftr_matcher(img1, img2, 1024)\n",
    "            mkpts1_loftr_lr, mkpts0_loftr_lr = loftr_matcher(img2, img1, 1024)\n",
    "            mkpts0_superglue_1024, mkpts1_superglue_1024 = superglue_matcher(img1, img2, 1024, tta_groups=[('orig', 'orig'),('flip_lr', 'flip_lr')])\n",
    "            mkpts0_superglue_1440, mkpts1_superglue_1440 = superglue_matcher(img1, img2, 1440, tta_groups=[('orig', 'orig'),('flip_lr', 'flip_lr')])\n",
    "            \n",
    "            mkpts0 = np.concatenate([mkpts0_loftr,mkpts0_superglue_1024,mkpts0_superglue_1440,mkpts0_loftr_lr], axis=0)\n",
    "            mkpts1 = np.concatenate([mkpts1_loftr,mkpts1_superglue_1024,mkpts1_superglue_1440,mkpts1_loftr_lr], axis=0)\n",
    "            \n",
    "            n_matches = len(mkpts1)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                 group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n",
    "#     clean_index_pairs = [(i,j) for pi, (i,j) in enumerate(index_pairs) if pi not in drop_pair]\n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return \n",
    "\n",
    "def match_superglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout_loftr',\n",
    "                   device=torch.device('cuda'),\n",
    "                   min_matches=15, resize_to_ = (640, 480)):\n",
    "\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            img1 = cv2.imread(fname1)\n",
    "            img2 = cv2.imread(fname2)\n",
    "            \n",
    "            mkpts0_superglue_1024, mkpts1_superglue_1024 = superglue_matcher_8096(img1, img2, 1024)\n",
    "            mkpts0_superglue_1440, mkpts1_superglue_1440 = superglue_matcher_8096(img1, img2, 1440)\n",
    "            \n",
    "            mkpts0 = np.concatenate([mkpts0_superglue_1024,mkpts0_superglue_1440], axis=0)\n",
    "            mkpts1 = np.concatenate([mkpts1_superglue_1024,mkpts1_superglue_1440], axis=0)\n",
    "            \n",
    "            n_matches = len(mkpts1)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                 group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n",
    "#     clean_index_pairs = [(i,j) for pi, (i,j) in enumerate(index_pairs) if pi not in drop_pair]\n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return \n",
    "\n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, 'simple-radial', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cfc16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.297616Z",
     "iopub.status.busy": "2023-06-11T11:58:24.297252Z",
     "iopub.status.idle": "2023-06-11T11:58:24.308802Z",
     "shell.execute_reply": "2023-06-11T11:58:24.307810Z"
    },
    "papermill": {
     "duration": 0.024469,
     "end_time": "2023-06-11T11:58:24.311385",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.286916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "src = '/kaggle/input/image-matching-challenge-2023'\n",
    "if DEBUG:\n",
    "    # Get data from csv.\n",
    "    csv_file = 'train/train_labels.csv'\n",
    "else:\n",
    "    csv_file = 'sample_submission.csv'\n",
    "data_dict = {}\n",
    "with open(f'{src}/{csv_file}', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            if DEBUG:\n",
    "                dataset, scene, image,  _, _ = l.strip().split(',')\n",
    "            else:\n",
    "                image, dataset, scene,  _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26521a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.331151Z",
     "iopub.status.busy": "2023-06-11T11:58:24.330573Z",
     "iopub.status.idle": "2023-06-11T11:58:24.336781Z",
     "shell.execute_reply": "2023-06-11T11:58:24.335647Z"
    },
    "papermill": {
     "duration": 0.0194,
     "end_time": "2023-06-11T11:58:24.339596",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.320196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2cfa01ab573141e4 / 2fa124afd1f74f38 -> 3 images\n"
     ]
    }
   ],
   "source": [
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a948472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.359475Z",
     "iopub.status.busy": "2023-06-11T11:58:24.359080Z",
     "iopub.status.idle": "2023-06-11T11:58:24.364748Z",
     "shell.execute_reply": "2023-06-11T11:58:24.363572Z"
    },
    "papermill": {
     "duration": 0.018421,
     "end_time": "2023-06-11T11:58:24.367349",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.348928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_results = {}\n",
    "timings = {\"shortlisting\":[],\n",
    "           \"feature_detection\": [],\n",
    "           \"feature_matching\":[],\n",
    "           \"RANSAC\": [],\n",
    "           \"Reconstruction\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a12fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.387213Z",
     "iopub.status.busy": "2023-06-11T11:58:24.386883Z",
     "iopub.status.idle": "2023-06-11T11:58:24.397661Z",
     "shell.execute_reply": "2023-06-11T11:58:24.396503Z"
    },
    "papermill": {
     "duration": 0.023419,
     "end_time": "2023-06-11T11:58:24.400145",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.376726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to create a submission file.\n",
    "def create_submission(out_results, data_dict):\n",
    "#     print(out_results,data_dict)\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print(image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad0278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.421148Z",
     "iopub.status.busy": "2023-06-11T11:58:24.420133Z",
     "iopub.status.idle": "2023-06-11T11:58:24.427049Z",
     "shell.execute_reply": "2023-06-11T11:58:24.425859Z"
    },
    "papermill": {
     "duration": 0.019692,
     "end_time": "2023-06-11T11:58:24.429355",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.409663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fc50d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.450562Z",
     "iopub.status.busy": "2023-06-11T11:58:24.449870Z",
     "iopub.status.idle": "2023-06-11T11:58:24.686387Z",
     "shell.execute_reply": "2023-06-11T11:58:24.685091Z"
    },
    "papermill": {
     "duration": 0.250028,
     "end_time": "2023-06-11T11:58:24.689099",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.439071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2cfa01ab573141e4\n",
      "2fa124afd1f74f38\n"
     ]
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    dataset_type = 'train'\n",
    "else:\n",
    "    dataset_type = 'test'\n",
    "    \n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    if dataset not in out_results:\n",
    "        out_results[dataset] = {}\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(scene)\n",
    "        # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "        # You may want to run this on the training data in that case?\n",
    "        img_dir = f'{src}/{dataset_type}/{dataset}/{scene}/images'\n",
    "        if not os.path.exists(img_dir):\n",
    "            continue\n",
    "        # Wrap the meaty part in a try-except block.\n",
    "        try:\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{dataset_type}/{x}' for x in data_dict[dataset][scene]]\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'/kaggle/temp/featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "            t=time()\n",
    "            index_pairs = get_image_pairs_shortlist(img_fnames,\n",
    "                                  sim_th = 0.5, # should be strict\n",
    "                                  min_pairs = 35, # we select at least min_pairs PER IMAGE with biggest similarity\n",
    "                                  exhaustive_if_less = 20,\n",
    "                                  device=device)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            t=time()\n",
    "\n",
    "            if len(index_pairs) >= 400:\n",
    "#                 detect_features(img_fnames, \n",
    "#                         2048*4,\n",
    "#                         feature_dir=feature_dir,\n",
    "#                         upright=False,\n",
    "#                         device=device,\n",
    "#                         resize_small_edge_to=1024\n",
    "#                         )\n",
    "#                 gc.collect()\n",
    "#                 t=time() -t \n",
    "#                 timings['feature_detection'].append(t)\n",
    "#                 print(f'Features detected in  {t:.4f} sec')\n",
    "#                 t=time()\n",
    "#                 match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device)\n",
    "                match_superglue(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n",
    "                mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "                mapper_options.min_model_size = 3\n",
    "                mapper_options.ba_local_max_refinements = 2\n",
    "#                 mapper_options.ba_global_max_refinements = 20\n",
    "            else:\n",
    "                match_loftr_superglue(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n",
    "                mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "                mapper_options.min_model_size = 3\n",
    "                mapper_options.ba_local_max_refinements = 2\n",
    "#                 mapper_options.ba_global_max_refinements = 10\n",
    "\n",
    "            t=time() -t \n",
    "            timings['feature_matching'].append(t)\n",
    "            print(f'Features matched in  {t:.4f} sec')\n",
    "            database_path = f'{feature_dir}/colmap.db'\n",
    "            if os.path.isfile(database_path):\n",
    "                os.remove(database_path)\n",
    "            gc.collect()\n",
    "            import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n",
    "            output_path = f'{feature_dir}/colmap_rec_{LOCAL_FEATURE}'\n",
    "\n",
    "            t=time()\n",
    "            pycolmap.match_exhaustive(database_path)\n",
    "            t=time() - t \n",
    "            timings['RANSAC'].append(t)\n",
    "            print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "            t=time()\n",
    "            # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "\n",
    "            \n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "\n",
    "            print(maps)\n",
    "            #clear_output(wait=False)\n",
    "            t=time() - t\n",
    "            timings['Reconstruction'].append(t)\n",
    "            print(f'Reconstruction done in  {t:.4f} sec')\n",
    "            imgs_registered  = 0\n",
    "            best_idx = None\n",
    "            print (\"Looking for the best reconstruction\")\n",
    "            if isinstance(maps, dict):\n",
    "                for idx1, rec in maps.items():\n",
    "                    print (idx1, rec.summary())\n",
    "                    if len(rec.images) > imgs_registered:\n",
    "                        imgs_registered = len(rec.images)\n",
    "                        best_idx = idx1\n",
    "            if best_idx is not None:\n",
    "                print (maps[best_idx].summary())\n",
    "                for k, im in maps[best_idx].images.items():\n",
    "                    key1 = f'{dataset}/{scene}/images/{im.name}'\n",
    "                    out_results[dataset][scene][key1] = {}\n",
    "                    out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n",
    "                    out_results[dataset][scene][key1][\"t\"] = deepcopy(im.tvec)\n",
    "\n",
    "            ##################################\n",
    "            if isinstance(maps, dict):\n",
    "                for idx1, rec in maps.items():\n",
    "                    poses = rec.images.items()\n",
    "                    for k, im in poses:\n",
    "                        key1 = f'{dataset}/{scene}/images/{im.name}'\n",
    "                        if key1 in out_results[dataset][scene]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            out_results[dataset][scene][key1] = {}\n",
    "                            out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n",
    "                            out_results[dataset][scene][key1][\"t\"] = deepcopy(im.tvec)\n",
    "\n",
    "            ##################################\n",
    "            print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n",
    "            print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "            create_submission(out_results, data_dict)\n",
    "        except:\n",
    "            pass        \n",
    "        gc.collect()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030024a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-11T11:58:24.712118Z",
     "iopub.status.busy": "2023-06-11T11:58:24.711711Z",
     "iopub.status.idle": "2023-06-11T11:58:24.718149Z",
     "shell.execute_reply": "2023-06-11T11:58:24.716925Z"
    },
    "papermill": {
     "duration": 0.021561,
     "end_time": "2023-06-11T11:58:24.721176",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.699615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_submission(out_results, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be1e78",
   "metadata": {
    "papermill": {
     "duration": 0.009015,
     "end_time": "2023-06-11T11:58:24.740186",
     "exception": false,
     "start_time": "2023-06-11T11:58:24.731171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.161859,
   "end_time": "2023-06-11T11:58:27.176585",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-11T11:57:55.014726",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
