{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision3D Tutorial: 3D Reconstruction from Images\n",
    "\n",
    "This notebook demonstrates how to use Vision3D for 3D reconstruction from a collection of images.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have installed Vision3D:\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "from vision3d import Vision3DPipeline\n",
    "from vision3d.models import LoFTRMatcher, SuperGlueMatcher\n",
    "from vision3d.utils.visualization import visualize_matches, visualize_reconstruction\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Images\n",
    "\n",
    "For this tutorial, we'll use a sample dataset. Replace with your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image directory\n",
    "image_dir = Path('./sample_images')  # Replace with your image directory\n",
    "image_paths = sorted(list(image_dir.glob('*.jpg')))\n",
    "\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(1, min(4, len(image_paths)), figsize=(20, 5))\n",
    "for i, img_path in enumerate(image_paths[:4]):\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[i].imshow(img_rgb)\n",
    "    axes[i].set_title(img_path.name)\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic 3D Reconstruction\n",
    "\n",
    "Let's perform a basic reconstruction using the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline\n",
    "pipeline = Vision3DPipeline(matcher_type='hybrid', device=device)\n",
    "\n",
    "# Run reconstruction\n",
    "reconstruction = pipeline.reconstruct(\n",
    "    [str(p) for p in image_paths],\n",
    "    output_dir='./output/basic',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reconstruction statistics\n",
    "stats = reconstruction['statistics']\n",
    "print(\"\\nReconstruction Statistics:\")\n",
    "print(f\"  Registered images: {stats['num_images']} / {len(image_paths)}\")\n",
    "print(f\"  3D points: {stats['num_points3D']}\")\n",
    "print(f\"  Mean reprojection error: {stats['mean_reprojection_error']:.3f} px\")\n",
    "print(f\"  Mean track length: {stats['mean_track_length']:.1f}\")\n",
    "\n",
    "if stats['point_cloud_bounds']:\n",
    "    bounds = stats['point_cloud_bounds']\n",
    "    print(f\"\\nPoint cloud bounds:\")\n",
    "    print(f\"  Min: {bounds['min']}\")\n",
    "    print(f\"  Max: {bounds['max']}\")\n",
    "    print(f\"  Center: {bounds['center']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Configuration\n",
    "\n",
    "Let's try reconstruction with custom settings optimized for quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom configuration for high-quality reconstruction\n",
    "custom_config = {\n",
    "    'image_resize': 1440,  # Higher resolution\n",
    "    'pair_selection': {\n",
    "        'min_pairs': 30,\n",
    "        'similarity_threshold': 0.5\n",
    "    },\n",
    "    'matching': {\n",
    "        'confidence_threshold': 0.3,\n",
    "        'use_tta': True,\n",
    "        'tta_variants': ['orig', 'flip_lr', 'rot_90']\n",
    "    },\n",
    "    'reconstruction': {\n",
    "        'min_model_size': 3,\n",
    "        'ba_refine_focal': True,\n",
    "        'ba_refine_principal': True,\n",
    "        'ba_refine_distortion': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize pipeline with custom config\n",
    "pipeline_custom = Vision3DPipeline(\n",
    "    matcher_type='hybrid',\n",
    "    device=device,\n",
    "    config=custom_config\n",
    ")\n",
    "\n",
    "# Run reconstruction\n",
    "reconstruction_custom = pipeline_custom.reconstruct(\n",
    "    [str(p) for p in image_paths],\n",
    "    output_dir='./output/custom',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Matching Visualization\n",
    "\n",
    "Let's visualize the feature matches between a pair of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two images to match\n",
    "img1_path = image_paths[0]\n",
    "img2_path = image_paths[1]\n",
    "\n",
    "# Initialize matcher\n",
    "matcher = LoFTRMatcher(device=device)\n",
    "\n",
    "# Match features\n",
    "kpts1, kpts2, conf = matcher.match_pair(str(img1_path), str(img2_path))\n",
    "\n",
    "print(f\"Found {len(kpts1)} matches\")\n",
    "print(f\"Mean confidence: {np.mean(conf):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matches\n",
    "img1 = cv2.imread(str(img1_path))\n",
    "img2 = cv2.imread(str(img2_path))\n",
    "\n",
    "# Draw matches\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Convert confidence to colors (red=low, green=high)\n",
    "colors = plt.cm.RdYlGn(conf)\n",
    "\n",
    "# Create side-by-side visualization\n",
    "h1, w1 = img1.shape[:2]\n",
    "h2, w2 = img2.shape[:2]\n",
    "vis = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
    "vis[:h1, :w1] = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "vis[:h2, w1:] = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Draw matches\n",
    "for i, (pt1, pt2) in enumerate(zip(kpts1, kpts2)):\n",
    "    color = tuple(int(c * 255) for c in colors[i][:3])\n",
    "    pt1 = tuple(pt1.astype(int))\n",
    "    pt2 = tuple((pt2 + [w1, 0]).astype(int))\n",
    "    cv2.line(vis, pt1, pt2, color, 1)\n",
    "    cv2.circle(vis, pt1, 3, color, -1)\n",
    "    cv2.circle(vis, pt2, 3, color, -1)\n",
    "\n",
    "plt.imshow(vis)\n",
    "plt.title(f\"{len(kpts1)} matches between {img1_path.name} and {img2_path.name}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "\n",
    "Export the reconstruction in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_dir = './output/export'\n",
    "pipeline.export_results(\n",
    "    reconstruction,\n",
    "    output_dir,\n",
    "    formats=['ply', 'json', 'colmap']\n",
    ")\n",
    "\n",
    "print(f\"Results exported to {output_dir}\")\n",
    "print(\"Files created:\")\n",
    "for f in Path(output_dir).glob('*'):\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced: Compare Different Matchers\n",
    "\n",
    "Let's compare the performance of different matchers on the same image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare matchers\n",
    "matchers = {\n",
    "    'LoFTR': LoFTRMatcher(device=device),\n",
    "    'SuperGlue': SuperGlueMatcher(device=device)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, matcher in matchers.items():\n",
    "    kpts1, kpts2, conf = matcher.match_pair(str(img1_path), str(img2_path))\n",
    "    results[name] = {\n",
    "        'num_matches': len(kpts1),\n",
    "        'mean_conf': np.mean(conf),\n",
    "        'median_conf': np.median(conf),\n",
    "        'high_conf': np.sum(conf > 0.5)\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results).T\n",
    "print(\"\\nMatcher Comparison:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tips for Best Results\n",
    "\n",
    "1. **Image Quality**: Use high-resolution images with good lighting\n",
    "2. **Overlap**: Ensure 60-80% overlap between consecutive images\n",
    "3. **Coverage**: Capture images from multiple viewpoints\n",
    "4. **Stability**: Avoid motion blur and use stable camera positions\n",
    "5. **Textures**: Include textured surfaces for better feature matching\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different matcher configurations\n",
    "- Experiment with test-time augmentation settings\n",
    "- Use the point cloud for downstream tasks (mesh generation, measurements, etc.)\n",
    "- Integrate with your own computer vision pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}