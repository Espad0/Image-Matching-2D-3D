{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "13b84d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import superglue\n",
    "from superglue.models.superpoint import SuperPoint\n",
    "from superglue.models.superglue import SuperGlue\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import os\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c6454664",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8677387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperGlueCustomMatchingV2(torch.nn.Module):\n",
    "    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n",
    "    def __init__(self, config={}, device=None):\n",
    "        super().__init__()\n",
    "        self.superpoint = SuperPoint(config.get('superpoint', {}))\n",
    "        self.superglue = SuperGlue(config.get('superglue', {}))\n",
    "\n",
    "        self.tta_map = {\n",
    "            'orig': self.untta_none,\n",
    "            'eqhist': self.untta_none,\n",
    "            'clahe': self.untta_none,\n",
    "            'flip_lr': self.untta_fliplr,\n",
    "            'flip_ud': self.untta_flipud,\n",
    "            'rot_r10': self.untta_rotr10,\n",
    "            'rot_l10': self.untta_rotl10,\n",
    "            'fliplr_rotr10': self.untta_fliplr_rotr10,\n",
    "            'fliplr_rotl10': self.untta_fliplr_rotl10\n",
    "        }\n",
    "        self.device = device\n",
    "\n",
    "    def forward_flat(self, data, ttas=['orig', ], tta_groups=[['orig']]):\n",
    "        \"\"\" Run SuperPoint (optionally) and SuperGlue\n",
    "        SuperPoint is skipped if ['keypoints0', 'keypoints1'] exist in input\n",
    "        Args:\n",
    "          data: dictionary with minimal keys: ['image0', 'image1']\n",
    "        \"\"\"\n",
    "        pred = {}\n",
    "\n",
    "        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n",
    "        # sp_st = time.time()\n",
    "        if 'keypoints0' not in data:\n",
    "            pred0 = self.superpoint({'image': data['image0']})\n",
    "            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n",
    "        if 'keypoints1' not in data:\n",
    "            pred1 = self.superpoint({'image': data['image1']})\n",
    "            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n",
    "        # sp_nd = time.time()\n",
    "        # print('SP:', sp_nd - sp_st, 's')\n",
    "\n",
    "        # Reverse-tta before inference\n",
    "        pred['scores0'] = list(pred['scores0'])\n",
    "        pred['scores1'] = list(pred['scores1'])\n",
    "        for i in range(len(pred['keypoints0'])):\n",
    "            pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i] = self.tta_map[ttas[i]](\n",
    "                pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i],\n",
    "                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=True)\n",
    "\n",
    "            pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i] = self.tta_map[ttas[i]](\n",
    "                pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i],\n",
    "                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=True)\n",
    "\n",
    "        # Batch all features\n",
    "        # We should either have i) one image per batch, or\n",
    "        # ii) the same number of local features for all images in the batch.\n",
    "        data = {**data, **pred}\n",
    "\n",
    "        group_preds = []\n",
    "        for tta_group in tta_groups:\n",
    "            group_mask = torch.from_numpy(np.array([x in tta_group for x in ttas], dtype=np.bool))\n",
    "            group_data = {\n",
    "                **{f'keypoints{k}': [data[f'keypoints{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n",
    "                **{f'descriptors{k}': [data[f'descriptors{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n",
    "                **{f'scores{k}': [data[f'scores{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n",
    "                **{f'image{k}': data[f'image{k}'][group_mask, ...] for k in [0, 1]},\n",
    "            }\n",
    "            for k, v in group_data.items():\n",
    "                if isinstance(group_data[k], (list, tuple)):\n",
    "                    if k.startswith('descriptor'):\n",
    "                        group_data[k] = torch.cat(group_data[k], 1)[None, ...]\n",
    "                    else:\n",
    "                        group_data[k] = torch.cat(group_data[k])[None, ...]\n",
    "                else:\n",
    "                    group_data[k] = torch.flatten(group_data[k], 0, 1)[None, ...]\n",
    "            # sg_st = time.time()\n",
    "            group_pred = {\n",
    "                # **{k: group_data[k] for k in group_data},\n",
    "                **group_data,\n",
    "                **self.superglue(group_data)\n",
    "            }\n",
    "            # sg_nd = time.time()\n",
    "            # print('SG:', sg_nd - sg_st, 's')\n",
    "            group_preds.append(group_pred)\n",
    "        return group_preds\n",
    "\n",
    "    def forward_cross(self, data, ttas=['orig', ], tta_groups=[('orig', 'orig')]):\n",
    "        pred = {}\n",
    "\n",
    "        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n",
    "        sp_st = time()\n",
    "        if 'keypoints0' not in data:\n",
    "            pred0 = self.superpoint({'image': data['image0']})\n",
    "            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n",
    "        if 'keypoints1' not in data:\n",
    "            pred1 = self.superpoint({'image': data['image1']})\n",
    "            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n",
    "        sp_nd = time()\n",
    "\n",
    "        # Batch all features\n",
    "        # We should either have i) one image per batch, or\n",
    "        # ii) the same number of local features for all images in the batch.\n",
    "        data = {**data, **pred}\n",
    "\n",
    "        # Group predictions (list, with elements with matches{0,1}, matching_scores{0,1} keys)\n",
    "        group_pred_list = []\n",
    "        tta2id = {k: i for i, k in enumerate(ttas)}\n",
    "        for tta_group in tta_groups:\n",
    "            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n",
    "            group_data = {\n",
    "                **{f'image{i}': data[f'image{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'descriptors{i}': data[f'descriptors{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "            }\n",
    "\n",
    "            for k in group_data:\n",
    "                if isinstance(group_data[k], (list, tuple)):\n",
    "                    group_data[k] = torch.stack(group_data[k])\n",
    "\n",
    "            group_sg_pred = self.superglue(group_data)\n",
    "            group_pred_list.append(group_sg_pred)\n",
    "\n",
    "        # UnTTA\n",
    "        data['scores0'] = list(data['scores0'])\n",
    "        data['scores1'] = list(data['scores1'])\n",
    "        for i in range(len(data['keypoints0'])):\n",
    "            data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i] = self.tta_map[ttas[i]](\n",
    "                data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i],\n",
    "                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=False)\n",
    "\n",
    "            data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i] = self.tta_map[ttas[i]](\n",
    "                data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i],\n",
    "                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=False)\n",
    "\n",
    "        # Sooo... groups?\n",
    "        for group_pred, tta_group in zip(group_pred_list, tta_groups):\n",
    "            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n",
    "            group_pred.update({\n",
    "                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n",
    "            })\n",
    "        return group_pred_list\n",
    "\n",
    "\n",
    "    def untta_none(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        if not inplace:\n",
    "            keypoints = keypoints.clone()\n",
    "        return keypoints, descriptors, scores\n",
    "    \n",
    "    def untta_fliplr(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        if not inplace:\n",
    "            keypoints = keypoints.clone()\n",
    "        keypoints[:, 0] = w - keypoints[:, 0] - 1.\n",
    "        return keypoints, descriptors, scores\n",
    "\n",
    "    def untta_flipud(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        if not inplace:\n",
    "            keypoints = keypoints.clone()\n",
    "        keypoints[:, 1] = h - keypoints[:, 1] - 1.\n",
    "        return keypoints, descriptors, scores\n",
    "\n",
    "    def untta_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is +10, inverse is -10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "\n",
    "    def untta_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is -10, inverse is +10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "        \n",
    "    def untta_fliplr_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is +10, inverse is -10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "\n",
    "    def untta_fliplr_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n",
    "        # rotr10 is -10, inverse is +10\n",
    "        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n",
    "        ones = torch.ones_like(keypoints[:, 0])\n",
    "        hom = torch.cat([keypoints, ones[:, None]], 1)\n",
    "        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n",
    "        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n",
    "        if mask_illegal:\n",
    "            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n",
    "            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n",
    "        else:\n",
    "            return rot_kpts, descriptors, scores\n",
    "\n",
    "\n",
    "class SuperGlueMatcherV2:\n",
    "    def __init__(self, config, device=None, conf_th=None):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self._superglue_matcher = SuperGlueCustomMatchingV2(\n",
    "            config=config, device=self.device,\n",
    "            ).eval().to(device)\n",
    "        self.conf_thresh = conf_th\n",
    "    \n",
    "    def prep_np_img(self, img, long_side=None):\n",
    "        if long_side is not None:\n",
    "            scale = long_side / max(img.shape[0], img.shape[1])\n",
    "            w = int(img.shape[1] * scale)\n",
    "            h = int(img.shape[0] * scale)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), scale\n",
    "    \n",
    "    def frame2tensor(self, frame):\n",
    "        return (torch.from_numpy(frame).float()/255.)[None, None].to(self.device)\n",
    "            \n",
    "    def tta_rotation_preprocess(self, img_np, angle):\n",
    "        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n",
    "        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n",
    "        rot_img = self.frame2tensor(cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0])))\n",
    "        return rot_M, rot_img, rot_M_inv\n",
    "\n",
    "    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n",
    "        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n",
    "        hom = np.concatenate([kpts, ones], 1)\n",
    "        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n",
    "        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n",
    "        return rot_kpts, mask\n",
    "# \n",
    "    def __call__(self, img_np0, img_np1, input_longside, tta_groups=[('orig', 'orig')], forward_type='cross'):\n",
    "        with torch.no_grad():\n",
    "            img_np0, scale0 = self.prep_np_img(img_np0, input_longside)\n",
    "            img_np1, scale1 = self.prep_np_img(img_np1, input_longside)\n",
    "\n",
    "            img_ts0 = self.frame2tensor(img_np0)\n",
    "            img_ts1 = self.frame2tensor(img_np1)\n",
    "            images0, images1 = [], []\n",
    "\n",
    "            tta = []\n",
    "            for tta_g in tta_groups:\n",
    "                tta += tta_g\n",
    "            tta = list(set(tta))\n",
    "\n",
    "            # TTA\n",
    "            for tta_elem in tta:\n",
    "                if tta_elem == 'orig':\n",
    "                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n",
    "                elif tta_elem == 'flip_lr':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n",
    "                elif tta_elem == 'flip_ud':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n",
    "                elif tta_elem == 'rot_r10':\n",
    "                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0, 15)\n",
    "                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1, 15)\n",
    "                elif tta_elem == 'rot_l10':\n",
    "                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0, -15)\n",
    "                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1, -15)\n",
    "                elif tta_elem == 'fliplr_rotr10':\n",
    "                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], 15)\n",
    "                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], 15)\n",
    "                elif tta_elem == 'fliplr_rotl10':\n",
    "                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], -15)\n",
    "                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], -15)\n",
    "                elif tta_elem == 'eqhist':\n",
    "                    img_ts0_aug = self.frame2tensor(cv2.equalizeHist(img_np0))\n",
    "                    img_ts1_aug = self.frame2tensor(cv2.equalizeHist(img_np1))\n",
    "                elif tta_elem == 'clahe':\n",
    "                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "                    img_ts0_aug = self.frame2tensor(clahe.apply(img_np0))\n",
    "                    img_ts1_aug = self.frame2tensor(clahe.apply(img_np1))\n",
    "                else:\n",
    "                    raise ValueError('Unknown TTA method.')\n",
    "\n",
    "                images0.append(img_ts0_aug)\n",
    "                images1.append(img_ts1_aug)\n",
    "\n",
    "            # Inference\n",
    "            if forward_type == 'cross':\n",
    "                pred = self._superglue_matcher.forward_cross(\n",
    "                    data={\n",
    "                        \"image0\": torch.cat(images0),\n",
    "                        \"image1\": torch.cat(images1)\n",
    "                    },\n",
    "                    ttas=tta, tta_groups=tta_groups)\n",
    "            elif forward_type == 'flat':\n",
    "                pred = self._superglue_matcher.forward_flat(\n",
    "                data={\n",
    "                    \"image0\": torch.cat(images0),\n",
    "                    \"image1\": torch.cat(images1)\n",
    "                },\n",
    "                ttas=tta, tta_groups=tta_groups)\n",
    "            else:\n",
    "                raise RuntimeError(f'Unknown forward_type {forward_type}')\n",
    "\n",
    "            mkpts0, mkpts1, mconf = [], [], []\n",
    "            for group_pred in pred:\n",
    "                pred_aug = {k: v[0].detach().cpu().numpy().squeeze() for k, v in group_pred.items()}\n",
    "                kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n",
    "                matches, conf = pred_aug[\"matches0\"], pred_aug[\"matching_scores0\"]\n",
    "\n",
    "                if self.conf_thresh is None:\n",
    "                    valid = matches > -1\n",
    "                else:\n",
    "                    valid = (matches > -1) & (conf >= self.conf_thresh)\n",
    "                mkpts0.append(kpts0[valid])\n",
    "                mkpts1.append(kpts1[matches[valid]])\n",
    "                mconf.append(conf[valid])\n",
    "\n",
    "            cat_mkpts0 = np.concatenate(mkpts0)\n",
    "            cat_mkpts1 = np.concatenate(mkpts1)\n",
    "            mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < img_np0.shape[1]) & (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < img_np0.shape[0])\n",
    "            mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < img_np1.shape[1]) & (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < img_np1.shape[0])\n",
    "            return cat_mkpts0[mask0 & mask1] / scale0, cat_mkpts1[mask0 & mask1] / scale1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "949f643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoFTRMatcher:\n",
    "    def __init__(self, device=None, input_longside=1200, conf_th=None):\n",
    "        self._loftr_matcher = KF.LoFTR(pretrained=None)\n",
    "        self._loftr_matcher.load_state_dict(torch.load(\"kornia/loftr_outdoor.ckpt\")['state_dict'])\n",
    "        self._loftr_matcher = self._loftr_matcher.to(device).eval()\n",
    "        self.device = device\n",
    "        self.conf_thresh = conf_th\n",
    "        \n",
    "    def prep_img(self, img, long_side=1200):\n",
    "        if long_side is not None:\n",
    "            scale = long_side / max(img.shape[0], img.shape[1]) \n",
    "            w = int(img.shape[1] * scale)\n",
    "            h = int(img.shape[0] * scale)\n",
    "            img = cv2.resize(img, (w, h))\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        img_ts = K.image_to_tensor(img, False).float() / 255.\n",
    "        img_ts = K.color.bgr_to_rgb(img_ts)\n",
    "        img_ts = K.color.rgb_to_grayscale(img_ts)\n",
    "        return img, img_ts.to(self.device), scale\n",
    "    \n",
    "    def tta_rotation_preprocess(self, img_np, angle):\n",
    "        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n",
    "        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n",
    "        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n",
    "\n",
    "        rot_img_ts = K.image_to_tensor(rot_img, False).float() / 255.\n",
    "        rot_img_ts = K.color.bgr_to_rgb(rot_img_ts)\n",
    "        rot_img_ts = K.color.rgb_to_grayscale(rot_img_ts)\n",
    "        return rot_M, rot_img_ts.to(self.device), rot_M_inv\n",
    "\n",
    "    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n",
    "        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n",
    "        hom = np.concatenate([kpts, ones], 1)\n",
    "        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n",
    "        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n",
    "        return rot_kpts, mask\n",
    "# \n",
    "    def __call__(self, img_np1, img_np2, input_longside, tta=['orig', 'flip_lr']):\n",
    "        with torch.no_grad():\n",
    "            img_np1, img_ts0, scale0 = self.prep_img(img_np1, input_longside)\n",
    "            img_np2, img_ts1, scale1 = self.prep_img(img_np2, input_longside)\n",
    "            images0, images1 = [], []\n",
    "\n",
    "            # TTA\n",
    "            for tta_elem in tta:\n",
    "                if tta_elem == 'orig':\n",
    "                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n",
    "                elif tta_elem == 'flip_lr':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n",
    "                elif tta_elem == 'flip_ud':\n",
    "                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n",
    "                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n",
    "                elif tta_elem == 'rot_r10':\n",
    "                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np1, 10)\n",
    "                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np2, 10)\n",
    "                elif tta_elem == 'rot_l10':\n",
    "                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np1, -10)\n",
    "                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np2, -10)\n",
    "                else:\n",
    "                    raise ValueError('Unknown TTA method.')\n",
    "                images0.append(img_ts0_aug)\n",
    "                images1.append(img_ts1_aug)\n",
    "\n",
    "            # Inference\n",
    "            input_dict = {\"image0\": torch.cat(images0), \"image1\": torch.cat(images1)}\n",
    "            correspondences = self._loftr_matcher(input_dict)\n",
    "            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "            batch_id = correspondences['batch_indexes'].cpu().numpy()\n",
    "            confidence = correspondences['confidence'].cpu().numpy()\n",
    "\n",
    "            # Reverse TTA\n",
    "            for idx, tta_elem in enumerate(tta):\n",
    "                batch_mask = batch_id == idx\n",
    "\n",
    "                if tta_elem == 'orig':\n",
    "                    pass\n",
    "                elif tta_elem == 'flip_lr':\n",
    "                    mkpts0[batch_mask, 0] = img_np1.shape[1] - mkpts0[batch_mask, 0]\n",
    "                    mkpts1[batch_mask, 0] = img_np2.shape[1] - mkpts1[batch_mask, 0]\n",
    "                elif tta_elem == 'flip_ud':\n",
    "                    mkpts0[batch_mask, 1] = img_np1.shape[0] - mkpts0[batch_mask, 1]\n",
    "                    mkpts1[batch_mask, 1] = img_np2.shape[0] - mkpts1[batch_mask, 1]\n",
    "                elif tta_elem == 'rot_r10':\n",
    "                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_r10_M0_inv)\n",
    "                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_r10_M1_inv)\n",
    "                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n",
    "                elif tta_elem == 'rot_l10':\n",
    "                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_l10_M0_inv)\n",
    "                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_l10_M1_inv)\n",
    "                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n",
    "                else:\n",
    "                    raise ValueError('Unknown TTA method.')\n",
    "                    \n",
    "            if self.conf_thresh is not None:\n",
    "                th_mask = confidence >= self.conf_thresh\n",
    "            else:\n",
    "                th_mask = confidence >= 0.\n",
    "            mkpts0, mkpts1 = mkpts0[th_mask, :], mkpts1[th_mask, :]\n",
    "\n",
    "            # Matching points\n",
    "            return mkpts0 / scale0, mkpts1 / scale1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f921b6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n",
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"outdoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "base_config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 3,\n",
    "        \"keypoint_threshold\": 0.005,\n",
    "        \"max_keypoints\": 2048,\n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\",\n",
    "        \"sinkhorn_iterations\": 100,\n",
    "        \"match_threshold\": 0.2,\n",
    "    }\n",
    "}\n",
    "f8000_config = {\n",
    "    \"superpoint\": {\n",
    "        \"nms_radius\": 3,\n",
    "        \"keypoint_threshold\": 0.005,\n",
    "        \"max_keypoints\": 2048*4,\n",
    "    },\n",
    "    \"superglue\": {\n",
    "        \"weights\": \"outdoor\",\n",
    "        \"sinkhorn_iterations\": 100,\n",
    "        \"match_threshold\": 0.2,\n",
    "    }\n",
    "}\n",
    "superglue_matcher = SuperGlueMatcherV2(base_config, device=device, conf_th=0.2)     \n",
    "superglue_matcher_8096 = SuperGlueMatcherV2(f8000_config, device=device, conf_th=0.2)     \n",
    "loftr_matcher = LoFTRMatcher(device=device, conf_th=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b31762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "\n",
    "    # if num_imgs <= exhaustive_if_less:\n",
    "    #     return get_img_pairs_exhaustive(fnames)\n",
    "    \n",
    "    model_path = ['./efficientnet/tf_efficientnet_b7.pth']\n",
    "    model_name = ['tf_efficientnet_b7']\n",
    "    descs_list = []\n",
    "    for i in range(len(model_name)): \n",
    "        model = timm.create_model(model_name[i], \n",
    "                                  checkpoint_path=model_path[i])\n",
    "        model.eval()\n",
    "        descs = get_global_desc(fnames, model, device=device)\n",
    "        descs_list.append(descs)\n",
    "        \n",
    "    descs = torch.cat(descs_list, dim=-1)\n",
    "    print(descs.shape)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ed8d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_desc(fnames, model, device=torch.device(\"cpu\")):\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    config   = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "\n",
    "    global_descs_convnext = []\n",
    "\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames), total=len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "\n",
    "        # ──── DEBUG 1: does the file exist? ────\n",
    "        if not os.path.isfile(img_fname_full):\n",
    "            print(f\"[DEBUG] {key}: file does NOT exist → {img_fname_full}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(img_fname_full)\n",
    "\n",
    "        # ──── DEBUG 2: did imread succeed? ────\n",
    "        if img is None:\n",
    "            print(f\"[DEBUG] {key}: cv2.imread() returned None\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"[DEBUG] {key}: loaded with shape {img.shape}\")\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (512, 512))\n",
    "\n",
    "        # ──── DEBUG 3: post-processing shape check ────\n",
    "        print(f\"[DEBUG] {key}: after resize → {img.shape}\")\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            desc     = model.forward_features(timg).mean(dim=(-1, 2))\n",
    "            desc     = desc.view(1, -1)\n",
    "            desc_lr  = model.forward_features(timg.flip(-1)).mean(dim=(-1, 2))\n",
    "            desc_lr  = desc_lr.view(1, -1)\n",
    "            desc_norm = F.normalize((desc + desc_lr) / 2, dim=1, p=2)\n",
    "\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ee9adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 3DOM_FBK_IMG_1537: loaded with shape (683, 1024, 3)\n",
      "[DEBUG] 3DOM_FBK_IMG_1537: after resize → (512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:04<00:17,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] .DS_Store: cv2.imread() returned None\n",
      "[DEBUG] 3DOM_FBK_IMG_1524: loaded with shape (683, 1024, 3)\n",
      "[DEBUG] 3DOM_FBK_IMG_1524: after resize → (512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:08<00:05,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 3DOM_FBK_IMG_1533: loaded with shape (683, 1024, 3)\n",
      "[DEBUG] 3DOM_FBK_IMG_1533: after resize → (512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:12<00:03,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] 3DOM_FBK_IMG_1528: loaded with shape (683, 1024, 3)\n",
      "[DEBUG] 3DOM_FBK_IMG_1528: after resize → (512, 512, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2560])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 5 but corresponding boolean dimension is 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(feature_dir):\n\u001b[1;32m      6\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(feature_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m index_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_pairs_shortlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_fnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43msim_th\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# should be strict\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmin_pairs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we select at least min_pairs PER IMAGE with biggest similarity\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mexhaustive_if_less\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m index_pairs\n",
      "Cell \u001b[0;32mIn[86], line 40\u001b[0m, in \u001b[0;36mget_image_pairs_shortlist\u001b[0;34m(fnames, sim_th, min_pairs, exhaustive_if_less, device)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m st_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_imgs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     39\u001b[0m     mask_idx \u001b[38;5;241m=\u001b[39m mask[st_idx]\n\u001b[0;32m---> 40\u001b[0m     to_match \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_match) \u001b[38;5;241m<\u001b[39m min_pairs:\n\u001b[1;32m     42\u001b[0m         to_match \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(dm[st_idx])[:min_pairs]  \n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 5 but corresponding boolean dimension is 4"
     ]
    }
   ],
   "source": [
    "img_dir = 'examples/images'\n",
    "img_fnames = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir)]\n",
    "\n",
    "feature_dir = './featureout'\n",
    "if not os.path.isdir(feature_dir):\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "index_pairs = get_image_pairs_shortlist(img_fnames,\n",
    "                        sim_th = 0.5, # should be strict\n",
    "                        min_pairs = 35, # we select at least min_pairs PER IMAGE with biggest similarity\n",
    "                        exhaustive_if_less = 20,\n",
    "                        device=device)\n",
    "\n",
    "index_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1e983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            if len(index_pairs) >= 400:\n",
    "#                 detect_features(img_fnames, \n",
    "#                         2048*4,\n",
    "#                         feature_dir=feature_dir,\n",
    "#                         upright=False,\n",
    "#                         device=device,\n",
    "#                         resize_small_edge_to=1024\n",
    "#                         )\n",
    "#                 gc.collect()\n",
    "#                 t=time() -t \n",
    "#                 timings['feature_detection'].append(t)\n",
    "#                 print(f'Features detected in  {t:.4f} sec')\n",
    "#                 t=time()\n",
    "#                 match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device)\n",
    "                match_superglue(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n",
    "                mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "                mapper_options.min_model_size = 3\n",
    "                mapper_options.ba_local_max_refinements = 2\n",
    "#                 mapper_options.ba_global_max_refinements = 20\n",
    "            else:\n",
    "                match_loftr_superglue(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n",
    "                mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "                mapper_options.min_model_size = 3\n",
    "                mapper_options.ba_local_max_refinements = 2\n",
    "#                 mapper_options.ba_global_max_refinements = 10\n",
    "\n",
    "            t=time() -t \n",
    "            timings['feature_matching'].append(t)\n",
    "            print(f'Features matched in  {t:.4f} sec')\n",
    "            database_path = f'{feature_dir}/colmap.db'\n",
    "            if os.path.isfile(database_path):\n",
    "                os.remove(database_path)\n",
    "            gc.collect()\n",
    "            import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n",
    "            output_path = f'{feature_dir}/colmap_rec_{LOCAL_FEATURE}'\n",
    "\n",
    "            t=time()\n",
    "            pycolmap.match_exhaustive(database_path)\n",
    "            t=time() - t \n",
    "            timings['RANSAC'].append(t)\n",
    "            print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "            t=time()\n",
    "            # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "\n",
    "            \n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "\n",
    "            print(maps)\n",
    "            #clear_output(wait=False)\n",
    "            t=time() - t\n",
    "            timings['Reconstruction'].append(t)\n",
    "            print(f'Reconstruction done in  {t:.4f} sec')\n",
    "            imgs_registered  = 0\n",
    "            best_idx = None\n",
    "            print (\"Looking for the best reconstruction\")\n",
    "            if isinstance(maps, dict):\n",
    "                for idx1, rec in maps.items():\n",
    "                    print (idx1, rec.summary())\n",
    "                    if len(rec.images) > imgs_registered:\n",
    "                        imgs_registered = len(rec.images)\n",
    "                        best_idx = idx1\n",
    "            if best_idx is not None:\n",
    "                print (maps[best_idx].summary())\n",
    "                for k, im in maps[best_idx].images.items():\n",
    "                    key1 = f'{dataset}/{scene}/images/{im.name}'\n",
    "                    out_results[dataset][scene][key1] = {}\n",
    "                    out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n",
    "                    out_results[dataset][scene][key1][\"t\"] = deepcopy(im.tvec)\n",
    "\n",
    "            ##################################\n",
    "            if isinstance(maps, dict):\n",
    "                for idx1, rec in maps.items():\n",
    "                    poses = rec.images.items()\n",
    "                    for k, im in poses:\n",
    "                        key1 = f'{dataset}/{scene}/images/{im.name}'\n",
    "                        if key1 in out_results[dataset][scene]:\n",
    "                            continue\n",
    "                        else:\n",
    "                            out_results[dataset][scene][key1] = {}\n",
    "                            out_results[dataset][scene][key1][\"R\"] = deepcopy(im.rotmat())\n",
    "                            out_results[dataset][scene][key1][\"t\"] = deepcopy(im.tvec)\n",
    "\n",
    "            ##################################\n",
    "            print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n",
    "            print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "            create_submission(out_results, data_dict)\n",
    "        except:\n",
    "            pass        \n",
    "        gc.collect()\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
